# src/ingest.py
# Script to ingest data from CSV files into the PostgreSQL database,
# corrected to match the provided schema and table names.

import pandas as pd
from src.database import get_db_connection, close_db_connection
from src.config import CANONICAL_ADDRESSES_CSV, TRANSACTIONS_CSV # Assuming these are defined
import logging
import psycopg2.extras # Required for execute_values
# Assume PostGIS is enabled if GEOMETRY is used in schema

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ingest_canonical_addresses(csv_path=CANONICAL_ADDRESSES_CSV):
    """
    Ingests canonical addresses from a CSV file into the database,
    matching the canonical_addresses schema (based on 'Address' headers).

    Args:
        csv_path (str): Path to the canonical addresses CSV file.
    """
    logging.info(f"Starting ingestion of canonical addresses from {csv_path}")
    conn = None # Initialize conn to None
    try:
        df = pd.read_csv(csv_path, sep=',')
        logging.info(f"Read {len(df)} rows from {csv_path}")

        # Prepare data to match the canonical_addresses schema columns:
        # hhid, fname, mname, lname, suffix, address, house, predir, street,
        # strtype, postdir, apttype, aptnbr, city, state, zip, latitude, longitude, homeownercd
        # address_id (BIGSERIAL, auto-generated by DB - do not include in INSERT)

        data = []
        for _, row in df.iterrows():
            # Map CSV column names to schema columns.
            # Use .get(column_name, None) to handle potentially missing CSV columns gracefully.
            # Adjust CSV column names ('...') if they differ from the schema headers.
            hhid = row.get('hhid', None)
            fname = row.get('fname', None)
            mname = row.get('mname', None)
            lname = row.get('lname', None)
            suffix = row.get('suffix', None)
            address = row.get('address', None) # Assuming 'address' in CSV maps to 'address' in schema
            house = row.get('house', None)
            predir = row.get('predir', None)
            street = row.get('street', None)
            strtype = row.get('strtype', None)
            postdir = row.get('postdir', None)
            apttype = row.get('apttype', None)
            aptnbr = row.get('aptnbr', None)
            city = row.get('city', None)
            state = row.get('state', None)
            # Ensure zip code is treated as string and handle potential NaNs
            zip_code = str(int(row['zip'])) if pd.notna(row.get('zip', None)) and isinstance(row.get('zip'), (int, float)) else str(row.get('zip', '')).strip() if pd.notna(row.get('zip')) else None
            latitude = row.get('latitude', None)
            longitude = row.get('longitude', None)
            homeownercd = row.get('homeownercd', None)


            data.append((
                hhid, fname, mname, lname, suffix, address, house, predir, street,
                strtype, postdir, apttype, aptnbr, city, state, zip_code, latitude,
                longitude, homeownercd
            ))


        # Correct INSERT query to match canonical_addresses schema columns (excluding address_id)
        insert_query = """
        INSERT INTO canonical_addresses (
            hhid, fname, mname, lname, suffix, address, house, predir, street,
            strtype, postdir, apttype, aptnbr, city, state, zip, latitude,
            longitude, homeownercd
        ) VALUES %s;
        """

        conn = get_db_connection()
        cur = conn.cursor()
        psycopg2.extras.execute_values(cur, insert_query, data, page_size=1000)
        conn.commit()
        logging.info(f"Successfully ingested {len(data)} canonical addresses.")

    except FileNotFoundError:
        logging.error(f"Error: Canonical addresses CSV not found at {csv_path}")
        raise
    except psycopg2.Error as e:
        logging.error(f"Database ingestion failed: {e}")
        if conn:
            conn.rollback()
        raise
    except Exception as e:
        logging.error(f"Unexpected error during canonical address ingestion: {e}")
        raise
    finally:
        if conn:
            close_db_connection(conn)



def ingest_transactions(csv_path=TRANSACTIONS_CSV):
    """
    Ingests transaction data from a CSV file into the database,
    matching the transactions schema.

    Args:
        csv_path (str): Path to the transactions CSV file.
    """
    logging.info(f"Starting ingestion of transactions from {csv_path}")
    conn = None
    try:
        df = pd.read_csv(csv_path, sep=',')
        logging.info(f"Read {len(df)} rows from {csv_path}")

        # Handle potential date values with explicit format to avoid warnings
        date_cols = ['created_at', 'updated_at', 'list_date', 'pending_date']
        for col in date_cols:
            if col in df.columns:
                # Convert to datetime with specific format, coerce errors to NaT
                # American date format MM/DD/YYYY is assumed based on sample data
                try:
                    df[col] = pd.to_datetime(df[col], format='%m/%d/%Y', errors='coerce')
                except Exception:
                    # Fall back to dateutil parser if format doesn't match
                    df[col] = pd.to_datetime(df[col], errors='coerce')

        data = []
        for _, row in df.iterrows():
            # Map CSV column names to schema columns
            id = row.get('id', None)
            status = row.get('status', None)
            price = row.get('price', None)
            bedrooms = row.get('bedrooms', None)
            bathrooms = row.get('bathrooms', None)
            square_feet = row.get('square_feet', None)
            address_line_1 = row.get('address_line_1', None)
            address_line_2 = row.get('address_line_2', None)
            city = row.get('city', None)
            state = row.get('state', None)
            # Ensure zip code is treated as string and handle potential NaNs
            zip_code = str(int(row['zip_code'])) if pd.notna(row.get('zip_code', None)) and isinstance(row.get('zip_code'), (int, float)) else str(row.get('zip_code', '')).strip() if pd.notna(row.get('zip_code')) else None
            property_type = row.get('property_type', None)
            year_built = row.get('year_built', None)
            presented_by = row.get('presented_by', None)
            brokered_by = row.get('brokered_by', None)
            presented_by_mobile = row.get('presented_by_mobile', None)
            mls = row.get('mls', None)
            listing_office_id = row.get('listing_office_id', None)
            listing_agent_id = row.get('listing_agent_id', None)

            # Handle Timestamp/Date conversion for DB
            # Convert pandas NaT to None for SQL compatibility
            created_at = None if pd.isna(row.get('created_at')) else row.get('created_at')
            updated_at = None if pd.isna(row.get('updated_at')) else row.get('updated_at')
            list_date = None if pd.isna(row.get('list_date')) else row.get('list_date')
            pending_date = None if pd.isna(row.get('pending_date')) else row.get('pending_date')

            open_house = row.get('open_house', None)
            latitude = row.get('latitude', None)
            longitude = row.get('longitude', None)
            email = row.get('email', None)

            presented_by_first_name = row.get('presented_by_first_name', None)
            presented_by_last_name = row.get('presented_by_last_name', None)
            presented_by_middle_name = row.get('presented_by_middle_name', None)
            presented_by_suffix = row.get('presented_by_suffix', None)

            # Handle geog column using PostGIS functions if latitude and longitude are available
            geog = row.get('geog', None)  # Try to use direct geog value if present
            
            # If no direct geog but we have lat/lon, prepare for PostGIS conversion
            if geog is None and pd.notna(latitude) and pd.notna(longitude):
                # We'll create a PostGIS point in the SQL query
                geog = f"ST_SetSRID(ST_MakePoint({longitude}, {latitude}), 4326)"
            else:
                geog = None

            data.append((
                id, status, price, bedrooms, bathrooms, square_feet, address_line_1,
                address_line_2, city, state, zip_code, property_type, year_built,
                presented_by, brokered_by, presented_by_mobile, mls, listing_office_id,
                listing_agent_id, created_at, updated_at, open_house, latitude, longitude,
                email, list_date, pending_date, presented_by_first_name,
                presented_by_last_name, presented_by_middle_name, presented_by_suffix, geog
            ))

        # Connect to DB and prepare cursor
        conn = get_db_connection()
        cur = conn.cursor()
        
        # Process records in batches to handle PostGIS points
        for i in range(0, len(data), 1000):
            batch = data[i:i+1000]
            batch_values = []
            
            # Create query parameters and handle PostGIS points
            for record in batch:
                values_list = list(record)
                # Check if geog is a PostGIS expression
                if isinstance(values_list[-1], str) and values_list[-1].startswith("ST_SetSRID"):
                    # Extract the geog expression for special handling
                    geog_expr = values_list[-1]
                    values_list[-1] = None  # Set to None for placeholder
                    # Create modified record tuple
                    modified_record = tuple(values_list)
                    batch_values.append(modified_record)
                    
                    # After batch insert, we'll update these records with PostGIS expression
                    cur.execute(f"""
                    UPDATE transactions 
                    SET geog = {geog_expr}
                    WHERE id = %s AND latitude = %s AND longitude = %s
                    """, (record[0], record[22], record[23]))
                else:
                    batch_values.append(record)
            
            # Only run batch insert if we have records
            if batch_values:
                # Correct INSERT query to match transactions schema columns
                insert_query = """
                INSERT INTO transactions (
                    id, status, price, bedrooms, bathrooms, square_feet, address_line_1,
                    address_line_2, city, state, zip_code, property_type, year_built,
                    presented_by, brokered_by, presented_by_mobile, mls, listing_office_id,
                    listing_agent_id, created_at, updated_at, open_house, latitude, longitude,
                    email, list_date, pending_date, presented_by_first_name,
                    presented_by_last_name, presented_by_middle_name, presented_by_suffix, geog
                ) VALUES %s;
                """
                
                psycopg2.extras.execute_values(cur, insert_query, batch_values, page_size=1000)
        
        conn.commit()
        logging.info(f"Successfully ingested {len(data)} transactions.")

    except FileNotFoundError:
        logging.error(f"Error: Transactions CSV not found at {csv_path}")
        raise
    except psycopg2.Error as e:
        logging.error(f"Database ingestion failed: {e}")
        if conn:
            conn.rollback()
        raise
    except Exception as e:
        logging.error(f"Unexpected error during transactions ingestion: {e}")
        raise
    finally:
        if conn:
            close_db_connection(conn)


if __name__ == "__main__":
    conn = None
    try:
        logging.info("Clearing existing data from canonical_addresses and transactions...")
        conn = get_db_connection()
        cur = conn.cursor()
        # Clear data from tables this script ingests into
        cur.execute("DELETE FROM transactions;")
        cur.execute("DELETE FROM canonical_addresses;")
        conn.commit()
        logging.info("Existing data from canonical_addresses and transactions cleared.")

        ingest_canonical_addresses()
        ingest_transactions()

        logging.info("Ingestion process completed.")

    except Exception as e:
        logging.error(f"Ingestion script failed: {e}")
    finally:
        if conn:
            close_db_connection(conn)